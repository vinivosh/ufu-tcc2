{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes de Desempenho\n",
    "\n",
    "## K-Means\n",
    "\n",
    "Vamos agora realizar testes de ganho de velocidade de execução, comparando o desempenho do K-Means rodando na CPU com o do K-means rodando na GPU.\n",
    "\n",
    "Desta vez, iremos utilizar datasets bem maiores e, portanto, nada triviais — como era o caso do [*Iris* Data Set](https://archive.ics.uci.edu/ml/datasets/Iris) que foi usado anteriormente apenas como uma prova de conceito e teste de corretude.\n",
    "\n",
    "A ideia é testar se os ganhos de desempenho ao utilizarmos uma versão paralelizada em GPU diminuem, estagnam ou aumentam junto com o aumento de instâncias ou dimensionalidade do dataset.\n",
    "\n",
    "Também será testado se houve diferença de precisção de cada classificação. Isso é realizado usando a função `getClassificationHits()`, explicada mais a fundo nos cadernos Jupyter `kMeansCPU.ipynb` e `kMeansGPU.ipynb`.\n",
    "\n",
    "### Código Comum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function kMeansCPU at 0x71994c0e82c0>\n",
      "<function kMeansGPU at 0x71994c0e9760>\n",
      "<function getClassificationHits at 0x719947f1b6a0>\n",
      "env: NUMBA_CUDA_LOW_OCCUPANCY_WARNINGS=0\n"
     ]
    }
   ],
   "source": [
    "import kMeans as km\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import importlib\n",
    "importlib.reload(km)\n",
    "\n",
    "# Testing imports\n",
    "print(km.kMeansCPU)\n",
    "print(km.kMeansGPU)\n",
    "print(km.getClassificationHits)\n",
    "\n",
    "# Se verdadeiro, os testes incluirão a contagem de acertos dos resultados dos algoritmos. Isso pode demorar MUITO (>5h por execução no dataset 5)!\n",
    "TEST_CORRECTEDNESS = True\n",
    "\n",
    "# Valor efetivamente infinito para um float, para ser usado como valor inicial na variável \"fastestExecTime\"\n",
    "FLOAT_MAX = float('inf')\n",
    "\n",
    "# Configurando Numba para não reportar erros de baixa ocupação dos streaming multiprocessors (SMs) da GPU\n",
    "# Não suprimir estes erros gera um overhead bem considerável, ocasionalmente, em algumas execuções do K-Means GPU\n",
    "%set_env NUMBA_CUDA_LOW_OCCUPANCY_WARNINGS 0\n",
    "\n",
    "# Função para rodar os testes\n",
    "def runTests(mode:str='GPU', runs:int=10, countHits:bool=True):\n",
    "    '''Essa função depende de diversas variáveis declaradas anteriormente. Portanto, é inútil fora deste caderno Jupyter!'''\n",
    "\n",
    "    mode = mode.upper()\n",
    "    if mode == 'CPU': kMeans = km.kMeansCPU\n",
    "    elif mode == 'GPU': kMeans = km.kMeansGPU\n",
    "    else: raise ValueError('Unknown mode!')\n",
    "\n",
    "    totalExecTime = 0.0\n",
    "    slowestExecTime = -1.0\n",
    "    fastestExecTime = FLOAT_MAX\n",
    "\n",
    "    totalHits = 0\n",
    "    totalHitsTime = 0.0\n",
    "\n",
    "    for rep in range(1, runs + 1):\n",
    "        startTime = time.perf_counter()\n",
    "        results = kMeans(datasetTreated, k=K, maxIter=MAX_ITERATIONS, printIter=False, plotResults=PLOT_RESULTS, debug=DEBUG)\n",
    "        elapsedTime = time.perf_counter() - startTime\n",
    "        if elapsedTime < fastestExecTime: fastestExecTime = elapsedTime\n",
    "        if elapsedTime > slowestExecTime: slowestExecTime = elapsedTime\n",
    "        totalExecTime += elapsedTime\n",
    "        print(f'Execution K-Means {mode} run #{rep}: {elapsedTime}; curr avg: {totalExecTime / rep}; ', end='')\n",
    "\n",
    "        if countHits:\n",
    "            # Verificando acertos\n",
    "            # Converting from numpy arrays to panda's dataframes, if needed\n",
    "            if results.__class__.__name__ != pd.DataFrame.__class__.__name__: results = pd.DataFrame(results)\n",
    "            startTime = time.perf_counter()\n",
    "            hits, _, _  = km.getClassificationHits(results, dataset, classColumnName, classes, debug=DEBUG)\n",
    "            elapsedTime = time.perf_counter() - startTime\n",
    "            totalHits += hits\n",
    "            totalHitsTime += elapsedTime\n",
    "            print(f'Hits: {hits} (done in {elapsedTime:.4f}); curr avg hits: {totalHits / rep}', end='')\n",
    "\n",
    "        print('\\n', end='')\n",
    "\n",
    "    print(f' \\nAvg exec K-Means {mode}: {totalExecTime / runs}')\n",
    "    print(f'Max exec K-Means {mode}: {slowestExecTime}')\n",
    "    print(f'Min exec time K-Means {mode}: {fastestExecTime}')\n",
    "\n",
    "    if countHits:\n",
    "        print(f' \\nAverage hits: {totalHits / runs}')\n",
    "        print(f'Avg exec K-Means {mode} classificationHits(): {totalHitsTime / runs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 (N > 1.000, D = 7, K = 2) — Rice (Cammeo and Osmancik)\n",
    "\n",
    "Foi utilizado aqui o Dataset **[Rice (Cammeo and Osmancik)](https://archive.ics.uci.edu/dataset/545/rice+cammeo+and+osmancik)**, que reúne dados expressando características morfológicas de grãos de arroz de duas espécies, extraídas a partir de fotos destes. Temos **7 variáveis (D = 7)** e **3.810 instâncias**.\n",
    "\n",
    "Esse dataset também contém informações de classe, definindo qual a espécie real do grão de arroz: **Cammeo** ou **Osmancik**. Portanto, haverão **2 grupos de dados (K = 2)**.\n",
    "\n",
    "Esse conjunto de dados está presente no arquivo `Rice_Cammeo_Osmancik.arff` dentro do arquivo `rice+cammeo+and+osmancik.zip` do dataset (também disponível em download direto [neste link](https://archive.ics.uci.edu/static/public/545/rice+cammeo+and+osmancik.zip)).\n",
    "\n",
    "#### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novas variáveis globais\n",
    "K = 2\n",
    "MAX_ITERATIONS = 60\n",
    "PLOT_RESULTS = False\n",
    "DEBUG = False\n",
    "\n",
    "COMMENT_CHAR = '%'\n",
    "ALTERNATIVE_COMMENT_CHARS = ['@']\n",
    "\n",
    "datasetFilePath = './Rice_Cammeo_Osmancik.csv'\n",
    "\n",
    "# Processando o aqruivo .arff file e convertendo para um arquivo .csv válido (com linhas comentadas)\n",
    "if not os.path.exists(datasetFilePath):\n",
    "    with \\\n",
    "        open('./rice+cammeo+and+osmancik/Rice_Cammeo_Osmancik.arff', 'r') as file,\\\n",
    "        open(datasetFilePath, 'w') as fileNew:\n",
    "\n",
    "        for line in file:\n",
    "            if line[0] in ALTERNATIVE_COMMENT_CHARS:\n",
    "                fileNew.write(COMMENT_CHAR + ' ' + line[1:])\n",
    "            else:\n",
    "                fileNew.write(line)\n",
    "\n",
    "columnNames = ['Area', 'Perimeter', 'Major_Axis_Length', 'Minor_Axis_Length', 'Eccentricity', 'Convex_Area', 'Extent', 'Class']\n",
    "\n",
    "# Lendo dataset do arquivo\n",
    "with open(datasetFilePath, 'r') as datasetFile:\n",
    "    dataset = pd.read_csv(datasetFilePath, names=columnNames, sep=',', skip_blank_lines=True, comment=COMMENT_CHAR)\n",
    "\n",
    "datasetTreated = dataset.drop(columns=['Class'])\n",
    "print(datasetTreated)\n",
    "\n",
    "classColumnName = 'Class'\n",
    "classes = dataset[classColumnName].unique()\n",
    "\n",
    "print(f'Classes (from column \"{classColumnName}\"): {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando o dataset (normalização min-max), para que todos valores estejam no intervalo [1, 10]\n",
    "datasetTreated = ((datasetTreated - datasetTreated.min()) / (datasetTreated.max() - datasetTreated.min())) * 9 + 1\n",
    "\n",
    "print(f'##### Dataset (tratado e normalizado, intervalo [1, 10]) #####\\n{datasetTreated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means CPU\n",
    "# * ####################################\n",
    "\n",
    "# runTests('CPU', 100, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means GPU\n",
    "# * ####################################\n",
    "\n",
    "\"\"\" NUMBER_OF_RUNS = 10\n",
    "\n",
    "totalExecTime = 0.0\n",
    "slowestExecTime = -1.0\n",
    "fastestExecTime = FLOAT_MAX\n",
    "\n",
    "totalHits = 0\n",
    "totalHitsTime = 0.0\n",
    "\n",
    "for rep in range(1, NUMBER_OF_RUNS + 1):\n",
    "    startTime = time.time()\n",
    "    results = km.kMeansGPU(datasetTreated, k=K, maxIter=MAX_ITERATIONS, printIter=False, plotResults=PLOT_RESULTS, debug=DEBUG)\n",
    "    elapsedTime = time.time() - startTime\n",
    "    if elapsedTime < fastestExecTime: fastestExecTime = elapsedTime\n",
    "    if elapsedTime > slowestExecTime: slowestExecTime = elapsedTime\n",
    "    totalExecTime += elapsedTime\n",
    "    print(f'Execution K-Means GPU run #{rep}: {elapsedTime}; curr avg: {totalExecTime / rep}; ', end='')\n",
    "\n",
    "    # Verificando acertos\n",
    "    # Converting from numpy arrays to panda's dataframes, if needed\n",
    "    if results.__class__.__name__ != pd.DataFrame.__class__.__name__: results = pd.DataFrame(results)\n",
    "    startTime = time.time()\n",
    "    hits, _, _  = km.getClassificationHits(results, dataset, classColumnName, classes, debug=DEBUG)\n",
    "    elapsedTime = time.time() - startTime\n",
    "    totalHits += hits\n",
    "    totalHitsTime += elapsedTime\n",
    "    print(f'Hits: {hits} (done in {elapsedTime:.4f}); curr avg hits: {totalHits / rep}\\n')\n",
    "\n",
    "print(f' \\nAvg exec K-Means GPU: {totalExecTime / NUMBER_OF_RUNS}')\n",
    "print(f'Max exec K-Means GPU: {slowestExecTime}')\n",
    "print(f'Min exec time K-Means GPU: {fastestExecTime}')\n",
    "\n",
    "print(f' \\nAverage hits: {totalHits / NUMBER_OF_RUNS}')\n",
    "print(f'Avg exec K-Means GPU classificationHits(): {totalHitsTime / NUMBER_OF_RUNS}') \"\"\"\n",
    "\n",
    "# runTests('GPU', 100, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 (N > 10.000, D = 8, K = 2) — HTRU2\n",
    "\n",
    "Foi utilizado aqui o Dataset **[HTRU2 (High Time Resolution Universe 2)](https://archive.ics.uci.edu/dataset/372/htru2)**, que reúne dados a respeito de emissões de sinais de rádio de banda larga obtidos através de leituras feitas com telescópios de rádio. É um dos resultados da busca por pulsares, estrelas de neutrôn que possuem uma rotação rápida e que emitem sinais de rádio banda larga detectáveis do nosso planeta. Temos **8 variáveis (D = 8)** e **17.898 instâncias**.\n",
    "\n",
    "Esse dataset também contém informações de classe, definindo se a leitura é **positiva** ou **negativa**, a respeito do sinal candidato de fato originar ou não de um pulsar. Portanto, haverão **2 grupos de dados (K = 2)**.\n",
    "\n",
    "Esse conjunto de dados está presente no arquivo `HTRU_2.csv` dentro do arquivo `HTRU_2.zip` do dataset (também disponível em download direto [neste link](https://archive.ics.uci.edu/static/public/372/htru2.zip)).\n",
    "\n",
    "#### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novas variáveis globais\n",
    "K = 2\n",
    "MAX_ITERATIONS = 60\n",
    "PLOT_RESULTS = False\n",
    "DEBUG = False\n",
    "\n",
    "datasetFilePath = './htru2/HTRU_2.csv'\n",
    "\n",
    "columnNames = ['mean_IP', 'std_dev_IP', 'exc_kurt_IP', 'skew_IP', 'mean_DM_SNR', 'std_dev_DM_SNR', 'exc_kurt_DM_SNR', 'skew_DM_SNR', 'is_positive']\n",
    "\n",
    "# Lendo dataset do arquivo\n",
    "with open(datasetFilePath, 'r') as datasetFile:\n",
    "    dataset = pd.read_csv(datasetFilePath, names=columnNames, sep=',')\n",
    "\n",
    "datasetTreated = dataset.drop(columns=['is_positive'])\n",
    "print(datasetTreated)\n",
    "\n",
    "classColumnName = 'is_positive'\n",
    "classes = dataset[classColumnName].unique()\n",
    "\n",
    "print(f'Classes (from column \"{classColumnName}\"): {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando o dataset (normalização min-max), para que todos valores estejam no intervalo [1, 10]\n",
    "datasetTreated = ((datasetTreated - datasetTreated.min()) / (datasetTreated.max() - datasetTreated.min())) * 9 + 1\n",
    "\n",
    "print(f'##### Dataset (tratado e normalizado, intervalo [1, 10]) #####\\n{datasetTreated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means CPU\n",
    "# * ####################################\n",
    "\n",
    "# runTests('CPU', 100, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means GPU\n",
    "# * ####################################\n",
    "\n",
    "# runTests('GPU', 100, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3 (N > 100.000, D = 50, K = 2) — MiniBooNE\n",
    "\n",
    "Foi utilizado aqui o Dataset **[MiniBooNE Particle Identification](https://archive.ics.uci.edu/dataset/199/miniboone+particle+identification)**, que reúne dados a respeito de partículas detectadas no experimento *MiniBooNE* (*Mini Booster Neutrino Experiment*), conduzido no laboratório americano *Fermilab*. Cada detecção de partícula é descrita por **50 variáveis reais (D = 50)** e há **129.596 instâncias no total**.\n",
    "\n",
    "As primeiras 36.488 instâncias são detecções de neutrinos do elétron (sinal) e as 93.108 restantes são de neutrinos do múon (ruído de fundo). Assim, as informações de classe desse dataset estão implícitas, expressa pela ordem das instâncias no arquivo. Como temos duas classes, haverão **2 grupos de dados (K = 2)**.\n",
    "\n",
    "Esse conjunto de dados está presente no arquivo `MiniBooNE_PID.txt` dentro do arquivo `miniboone+particle+identification.zip` do dataset (também disponível em download direto [neste link](https://archive.ics.uci.edu/static/public/199/miniboone+particle+identification.zip)).\n",
    "\n",
    "Foi necessário, neste dataset, realizar um **pré-processamento** para **remoção de outliers**. Originalmente, há 130.064 instâncias no total (36.499 sinal e 93.565 ruído). Porém, existem 468 instâncias (11 sinal e 457 ruído) que são extremos outliers, possuindo o valor -999.0 em todas as 50 variáveis — provavelmente advindos de algum erro de detecção. A presença destes outliers causava a criação de um cluster contendo apenas estes outliers, diminuindo muito o tempo de execução do algoritmo de maneira artificial. Estes outliers tiveram que ser removidos. Note que poderíamos ter solucionado este problema com outra abordagem: aumentar K para 3, criando um cluster novo para conter apenas os outliers. Isso, no entanto, seria mais custoso computacionalmente do que a remoção das instâncias.\n",
    "\n",
    "#### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novas variáveis globais\n",
    "K = 2\n",
    "MAX_ITERATIONS = 60\n",
    "PLOT_RESULTS = False\n",
    "DEBUG = False\n",
    "\n",
    "COMMENT_CHAR = '#'\n",
    "\n",
    "# As primeiras 36.499 instâncias são consideradas um sinal, e o resto como ruído\n",
    "N_OF_SIGNAL_LINES = 36499\n",
    "\n",
    "datasetFilePath = './MiniBooNE_PID.csv'\n",
    "\n",
    "# Processando o aqruivo .txt file e convertendo para um arquivo .csv válido (com a primeira linha comentada, removendo o leading whitespace, e trocando o separador de \"  \" ou \" \" para \",\")\n",
    "if not os.path.exists(datasetFilePath):\n",
    "    with \\\n",
    "        open('./MiniBooNE_PID.txt', 'r') as file,\\\n",
    "        open(datasetFilePath, 'w') as fileNew:\n",
    "\n",
    "        print('Processing MiniBooNE_PID.txt...\\n ')\n",
    "\n",
    "        # Removendo outliers com -999.0 de valor nas 50 variáveis. Há 468 destas instâncias\n",
    "        outlierString = '''-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03,-0.999000E+03'''\n",
    "\n",
    "        index = 1\n",
    "        signalInstRemoved = 0\n",
    "        noiseInstRemoved = 0\n",
    "        for line in file:\n",
    "            if index != 1:\n",
    "                lineToWrite = line.strip(' ').replace('  ', ' ').replace(' ', ',')\n",
    "                if outlierString not in lineToWrite:\n",
    "                    fileNew.write(lineToWrite)\n",
    "                else:\n",
    "                    if index - 1 <= N_OF_SIGNAL_LINES:\n",
    "                        # print(f'Instance (signal) #{index - 1} removed...')\n",
    "                        signalInstRemoved += 1\n",
    "                    else:\n",
    "                        # print(f'Instance (noise) #{index - 1} removed...')\n",
    "                        noiseInstRemoved += 1\n",
    "            # else:\n",
    "            #     fileNew.write(COMMENT_CHAR + ' ' + line.strip(' ').replace('  ', ' '))\n",
    "            index += 1\n",
    "\n",
    "        print(f'Signal outlier instances removed = {signalInstRemoved}')\n",
    "        print(f'Noise outlier instances removed = {noiseInstRemoved}\\n ')\n",
    "\n",
    "        print(f'Processed dataset saved in {datasetFilePath} with success!\\n ')\n",
    "else:\n",
    "    print(f'Processed dataset found in {datasetFilePath}. No need for processing!\\n ')\n",
    "\n",
    "columnNames = [f'id_var_{i}' for i in range(1, 50 + 1) ]\n",
    "\n",
    "# Lendo dataset do arquivo\n",
    "with open(datasetFilePath, 'r') as datasetFile:\n",
    "    dataset = pd.read_csv(datasetFile, names=columnNames, sep=',', skip_blank_lines=True)\n",
    "\n",
    "# Gerando coluna de classes\n",
    "classColumn = pd.DataFrame(['signal' if idx <= 36488 else 'noise' for idx in range(1, len(dataset) + 1)])\n",
    "# print(classColumn)\n",
    "\n",
    "classColumnName = 'class'\n",
    "dataset.insert(len(dataset.columns), classColumnName, classColumn)\n",
    "del classColumn\n",
    "\n",
    "datasetTreated = dataset.drop(columns=[classColumnName])\n",
    "print(datasetTreated)\n",
    "\n",
    "classes = ['signal', 'noise']\n",
    "print(f'Classes (from column \"{classColumnName}\"): {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando o dataset (normalização min-max), para que todos valores estejam no intervalo [1, 10]\n",
    "datasetTreated = ((datasetTreated - datasetTreated.min()) / (datasetTreated.max() - datasetTreated.min())) * 9 + 1\n",
    "\n",
    "print(f'##### Dataset (tratado e normalizado, intervalo [1, 10]) #####\\n{datasetTreated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means CPU\n",
    "# * ####################################\n",
    "\n",
    "# runTests('CPU', 50, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means GPU\n",
    "# * ####################################\n",
    "\n",
    "# runTests('GPU', 50, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4 (N > 1.000.000, D = 8) — WESAD\n",
    "\n",
    "Foi utilizado aqui um sub-conjunto dos dados do Dataset **[WESAD (Wearable Stress and Affect Detection)](https://archive.ics.uci.edu/dataset/465/wesad+wearable+stress+and+affect+detection)**, que reúne dados, fisiológicos e de movimento, de diversos sensores presentes em aparelhos *wearables* usados por 15 pacientes diferentes em testes laboratoriais. Um aparelho foi usado no peitoral e outro no pulso dos pacientes.\n",
    "\n",
    "Esse dataset também contém informações de classe, definindo momentos dos testes como pertencendo à três classificações de emoção do paciente: **referência**, **estresse** ou **diversão**. Portanto, haverão **3 grupos de dados (K = 3)**.\n",
    "\n",
    "O sub-conjunto de dados utilizado foi: dados obtidos apenas através do **aparelho usado no peito** do paciente, e apenas do **paciente #4**. Utilizando este sub-conjunto, temos **8 variáveis (D = 8)** e **4.588.552 instâncias**, cada uma sendo uma leitura ao longo do tempo do teste laboratorial (leituras realizadas na frequência de 700hz).\n",
    "\n",
    "Esse sub-conjunto de dados está presente no arquivo `S4/S4_respiban.txt` dentro do arquivo `WESAD.zip` do dataset (também disponível em download direto [neste link](https://uni-siegen.sciebo.de/s/HGdUkoNlW1Ub0Gx/download)).\n",
    "\n",
    "#### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novas variáveis globais\n",
    "K = 3\n",
    "MAX_ITERATIONS = 60\n",
    "PLOT_RESULTS = False\n",
    "DEBUG = False\n",
    "\n",
    "datasetFilePath = './WESAD/S4/S4_respiban.txt'\n",
    "columnNames = ['index', 'DI', 'ECG', 'EDA', 'EMG', 'TEMP', 'spatialX', 'spatialY', 'spatialZ', 'RESPIRATION', '_ignore_']\n",
    "\n",
    "# Lendo dataset do arquivo\n",
    "with open(datasetFilePath, 'r') as datasetFile:\n",
    "    dataset = pd.read_csv(datasetFilePath, names=columnNames, sep='\\t', index_col=0, skip_blank_lines=True, comment='#')\n",
    "\n",
    "# datasetTreated = dataset.drop(columns=['DI', '_ignore_'])\n",
    "# print(datasetTreated)\n",
    "\n",
    "# classColumnName = 'DI'\n",
    "# classes = dataset[classColumnName].unique()\n",
    "\n",
    "# print(f'Classes (from column \"{classColumnName}\"): {classes}')\n",
    "\n",
    "# Gerando coluna de classes\n",
    "classColumn = []\n",
    "for idx in range(len(dataset)):\n",
    "    classification = None\n",
    "    if idx < 1329300: classification = 'base'\n",
    "    elif idx < 1926400: classification = 'fun'\n",
    "    elif idx < 2563400: classification = 'base' # Medi 1\n",
    "    elif idx < 4020100: classification = 'stress'\n",
    "    else: classification = 'base' # Medi 2\n",
    "\n",
    "    classColumn.append(classification)\n",
    "classColumn = pd.DataFrame(classColumn)\n",
    "print(classColumn)\n",
    "\n",
    "classColumnName = 'class'\n",
    "dataset.insert(len(dataset.columns), classColumnName, classColumn)\n",
    "del classColumn\n",
    "\n",
    "datasetTreated = dataset.drop(columns=['DI', '_ignore_', classColumnName])\n",
    "print(datasetTreated)\n",
    "\n",
    "classes = ['base', 'fun', 'stress']\n",
    "print(f'Classes (from column \"{classColumnName}\"): {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando o dataset (normalização min-max), para que todos valores estejam no intervalo [1, 10]\n",
    "datasetTreated = ((datasetTreated - datasetTreated.min()) / (datasetTreated.max() - datasetTreated.min())) * 9 + 1\n",
    "\n",
    "print(f'##### Dataset (tratado e normalizado, intervalo [1, 10]) #####\\n{datasetTreated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means CPU\n",
    "# * ####################################\n",
    "\n",
    "# runTests('CPU', 5, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means GPU\n",
    "# * ####################################\n",
    "\n",
    "# runTests('GPU', 5, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados\n",
    "\n",
    "> Resultados completos disponíveis no arquivo `code/examples-and-tests/speedupTestsRawResults.txt`\n",
    "\n",
    "| |Tempo médio (50 execuções)|Speedup Médio|\n",
    "|-|-|-|\n",
    "|K-Means CPU|~129,81s|-|\n",
    "|K-Means GPU|~27,87s|~4,65x|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 5 (N > 10.000.000, D = 3, K = 7) — HHAR\n",
    "\n",
    "Foi utilizado aqui um sub-conjunto dos dados do Dataset **[Heterogeneity Human Activity Recognition (HHAR)](https://archive.ics.uci.edu/dataset/344/heterogeneity+activity+recognition)**, que reúne dados de movimento do giroscópio e acelerômetro presentes em aparelhos celulares (*smartphones*) e relógios (*smartwatches*) usados por 9 usuários diferentes ao realizar diversas atividades físicas diferentes ou estando em repouso.\n",
    "\n",
    "Esse dataset também contém informações de classe, definindo momentos dos testes como pertencendo a uma de seis atividades realizadas: **ciclismo**, **repouso (sentado)**, **repouso (em pé)**, **andar**, **subir escadas** e **descer escadas**. Além disto, há uma sétima \"atividade\", **nula**, que representa espaços do teste onde não foi realizada nenhuma atividade. Portanto, haverão **7 grupos de dados (K = 7)**.\n",
    "\n",
    "O sub-conjunto de dados utilizado foi: dados obtidos apenas através do **giroscópio do smartphone** do usuário. Utilizando este sub-conjunto, temos **3 variáveis (D = 3)** e **13.932.632 instâncias**, cada uma sendo uma leitura ao longo do tempo do experimento.\n",
    "\n",
    "Esse sub-conjunto de dados está presente no arquivo `Activity recognition exp/Phones_gyroscope.csv` dentro do arquivo `heterogeneity+activity+recognition.zip` do dataset (também disponível em download direto [neste link](https://archive.ics.uci.edu/static/public/344/heterogeneity+activity+recognition.zip)).\n",
    "\n",
    "#### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novas variáveis globais\n",
    "K = 7\n",
    "MAX_ITERATIONS = 60\n",
    "PLOT_RESULTS = False\n",
    "DEBUG = False\n",
    "\n",
    "datasetFilePath = './heterogeneity+activity+recognition/Activity recognition exp/Phones_gyroscope.csv'\n",
    "columnNames = ['index', 'arrival_time', 'creation_Time', 'x', 'y', 'z', 'user', 'model', 'device', 'gt']\n",
    "\n",
    "# Lendo dataset do arquivo\n",
    "with open(datasetFilePath, 'r') as datasetFile:\n",
    "    dataset = pd.read_csv(datasetFile, names=columnNames, header=0, sep=',', index_col=0)\n",
    "\n",
    "datasetTreated = dataset.drop(columns=['arrival_time', 'creation_Time', 'user', 'model', 'device', 'gt'])\n",
    "print(datasetTreated)\n",
    "\n",
    "classColumnName = 'gt'\n",
    "classes = dataset[classColumnName].unique()\n",
    "\n",
    "print(f'Classes (from column \"{classColumnName}\"): {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando o dataset (normalização min-max), para que todos valores estejam no intervalo [1, 10]\n",
    "datasetTreated = ((datasetTreated - datasetTreated.min()) / (datasetTreated.max() - datasetTreated.min())) * 9 + 1\n",
    "\n",
    "print(f'##### Dataset (tratado e normalizado, intervalo [1, 10]) #####\\n{datasetTreated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means CPU\n",
    "# * ####################################\n",
    "\n",
    "runTests('CPU', 1, TEST_CORRECTEDNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ####################################\n",
    "# * Rodando o K-Means GPU\n",
    "# * ####################################\n",
    "\n",
    "runTests('GPU', 2, TEST_CORRECTEDNESS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
