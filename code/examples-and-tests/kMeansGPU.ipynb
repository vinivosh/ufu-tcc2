{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando implementação em GPU do k-means\n",
    "\n",
    "Em grande parte baseada na implementação disponível em [dataquestio/kmeans](https://github.com/dataquestio/project-walkthroughs/tree/master/kmeans). Código disponível na função `kMeansGPU`, localizada no arquivo `kMeans.py`.\n",
    "\n",
    "## Imports e Variáveis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "PLOT_RESULTS = True\n",
    "DEBUG = True\n",
    "\n",
    "N = 100000\n",
    "D = 5\n",
    "\n",
    "K = 3\n",
    "MAX_ITERATIONS = 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o Dataset\n",
    "\n",
    "- `N` datapoints\n",
    "- Datapoints `D`-dimensionais\n",
    "- Floats randomizados ➡ intervalo de [1, 10) (ou [1, 10], dependendo da método de arredondamento de float usado. Isto não deve ter nenhuma relevância estatística, no entanto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = [[np.random.uniform(1, 10) for _ in range(D)] for _ in range(N)]\n",
    "dataset = pd.DataFrame(dataset, columns=[f'd{i}' for i in range(D)])\n",
    "\n",
    "# # # ! Descomente para exibir\n",
    "# dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * ############################################################################\n",
    "# * Funções auxiliares paralelizadas\n",
    "# * \n",
    "# * Essas são funções vetorizadas que rodarão na GPU. Contém as partes do K-means que mais demandam poder computacional\n",
    "# * ############################################################################\n",
    "\n",
    "@numba.guvectorize(\n",
    "    ['void(float64[:,:], float64[:], float64[:])'],\n",
    "    '(k,d),(d)->(k)',\n",
    "    nopython=True,\n",
    "    target='cuda'\n",
    ")\n",
    "def calcDistances(centroids:list[list[np.float64]], rowDataset:list[np.float64], rowResults:list[np.float64]):\n",
    "    d = len(rowDataset) # Dimensionality\n",
    "\n",
    "    for centroidIndex, centroid in enumerate(centroids):\n",
    "        distance = 0.0\n",
    "        for dim in range(d): distance += (rowDataset[dim] - centroid[dim]) ** 2\n",
    "        distance = distance ** (1/2)\n",
    "\n",
    "        rowResults[centroidIndex] = distance\n",
    "\n",
    "\n",
    "@numba.guvectorize(\n",
    "    ['void(float64[:],int64[:])'],\n",
    "    '(k)->()',\n",
    "    nopython=True,\n",
    "    target='cuda'\n",
    ")\n",
    "def calcClosestCentroids(rowDistances:list[np.float64], closestCent:np.int64):\n",
    "    minDistance = rowDistances[0]\n",
    "    minDistanceIndex = 0\n",
    "\n",
    "    # Retornar o index do valor mínimo em rowDistances\n",
    "    for index, distance in enumerate(rowDistances):\n",
    "        if distance < minDistance:\n",
    "            minDistance = distance\n",
    "            minDistanceIndex = index\n",
    "\n",
    "    closestCent[0] = minDistanceIndex\n",
    "\n",
    "\n",
    "@numba.guvectorize(\n",
    "    ['void(float64[:],float64[:])'],\n",
    "    '(d)->(d)',\n",
    "    nopython=True,\n",
    "    target='cuda'\n",
    ")\n",
    "def calcLogs(rowDataset:list[np.float64], rowResults:list[np.float64]):\n",
    "    # Calcular o log natural de cada dimensão do datapoint\n",
    "    for dimIdx, dimValue in enumerate(rowDataset): rowResults[dimIdx] = math.log(dimValue)\n",
    "\n",
    "\n",
    "\n",
    "# * ############################################################################\n",
    "\n",
    "def kMeansGPU(dataset:pd.DataFrame, k=3, maxIter=100, plotResults=False, debug=False):\n",
    "    if plotResults:\n",
    "        # Inicializando variáveis para exibição gráfica\n",
    "        pca = PCA(n_components=2) # dois eixos no gráfico\n",
    "        dataset_2D = pca.fit_transform(dataset.values)\n",
    "\n",
    "    n = len(dataset)\n",
    "    d = len(dataset.iloc[0])\n",
    "\n",
    "    # Gerando centróides iniciais randomicamente\n",
    "    centroids:pd.DataFrame = pd.concat([(dataset.apply(lambda x: float(x.sample().iloc[0]))) for _ in range(k)], axis=1) # * Paralelizar isto provavelmente é irrelevante, visto que sempre teremos poucos centróides\n",
    "    centroids_OLD = pd.DataFrame()\n",
    "\n",
    "    centroids__np = centroids.T.to_numpy()\n",
    "    centroids_OLD__np = centroids_OLD.T.to_numpy()\n",
    "    dataset__np = dataset.to_numpy()\n",
    "\n",
    "    iteration = 1\n",
    "\n",
    "    while iteration <= maxIter and not np.array_equal(centroids_OLD__np ,centroids__np):\n",
    "        if plotResults or debug: clear_output(wait=True)\n",
    "        if debug: debugStr = f'Iteration {iteration}\\n\\nCentroids:\\n{centroids.T}\\n\\n'\n",
    "\n",
    "        # Para cada datapoint, calcular distâncias entre ele e cada centróide; depois, encontrar o centróide mais próximo e salvar seu index\n",
    "        distances = np.zeros((n, k))\n",
    "        calcDistances(centroids__np, dataset__np, distances)\n",
    "\n",
    "        if debug: debugStr += f'Distances:\\n{distances}\\n\\n'\n",
    "\n",
    "        closestCent = np.zeros(n, np.int64)\n",
    "        calcClosestCentroids(distances, closestCent)\n",
    "        del distances\n",
    "        if debug: debugStr += f'Closest centroid index:\\n{closestCent}\\n\\n'\n",
    "\n",
    "        centroids_OLD__np = centroids__np.copy()\n",
    "\n",
    "        datasetLogs = np.zeros((n, d))\n",
    "        calcLogs(dataset__np, datasetLogs)\n",
    "\n",
    "        # meansByClosestCent[0] = médias dos logs de todos datapoints cujo centróide mais próximo é o centróide de index zero\n",
    "        meansByClosestCent = np.zeros((k, d))\n",
    "\n",
    "        for centroidIdx in range(k):\n",
    "            x = [(True if closestCent[dpIdx] == centroidIdx else False) for dpIdx in range(n)]\n",
    "            # relevantLogs conterá agora todos itens de datasetLogs cujo datapoint correspondente está mais próximo do centróide de index centroidIdx\n",
    "            relevantLogs = datasetLogs[x]\n",
    "            del x\n",
    "            meansByClosestCent[centroidIdx] = relevantLogs.mean(axis=0)\n",
    "            del relevantLogs\n",
    "\n",
    "            centroids__np[centroidIdx] = np.exp(meansByClosestCent[centroidIdx])\n",
    "\n",
    "        if plotResults:\n",
    "            # Plotando clusters\n",
    "            centroids_2D = pca.transform(centroids__np)\n",
    "            plt.title(f'Iteration {iteration}')\n",
    "            plt.scatter(x=dataset_2D[:,0], y=dataset_2D[:,1], c=closestCent)\n",
    "            plt.scatter(x=centroids_2D[:,0], y=centroids_2D[:,1], marker='+', linewidths=2, color='red')\n",
    "            plt.show()\n",
    "\n",
    "        if debug: print(debugStr)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return closestCent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ! Descomente para rodar de fato\n",
    "# result = kMeansGPU(dataset, K, MAX_ITERATIONS, PLOT_RESULTS, DEBUG)\n",
    "\n",
    "# result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando com um Dataset Real (Pequeno) — *Iris*\n",
    "\n",
    "Vamos agora testar esta implementação do k-means usando um dataset real de tamanho bem trivial.\n",
    "\n",
    "Utilizaremos o [*Iris* Data Set](https://archive.ics.uci.edu/ml/datasets/Iris), que consiste de dados a respeito de espécimes de [flores do gênero Íris](https://pt.wikipedia.org/wiki/Iris_(g%C3%A9nero)), popularmente chamadas de **Lírios**.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "O *Iris* consiste em **150 instâncias**, com dados multivariados de **quatro atributos**:\n",
    "\n",
    "1. Comprimento da sépala em centímetros\n",
    "2. Largura da sépala em centímetros\n",
    "3. Comprimento da pétala em centímetros\n",
    "4. Largura da pétala em centímetros\n",
    "\n",
    "Há também um quinto atributo de **Classe**, o **tipo da planta**, que é o que esperamos predizer utilizando o algoritmo. Há **três classes** neste dataset:\n",
    "\n",
    "- Iris Setosa\n",
    "- Iris Versicolour\n",
    "- Iris Virginica\n",
    "\n",
    "Assim, temos nossas variáveis para execução do k-means:\n",
    "\n",
    "- `N` = 150\n",
    "- `D` = 4\n",
    "- `K` = 3\n",
    "\n",
    "### Carregando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from os.path import exists as os_path_exists\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "\n",
    "# Novas variáveis globais\n",
    "MAX_ITERATIONS = 100\n",
    "PLOT_RESULTS = True\n",
    "DEBUG = True\n",
    "# ? K será inferido pela quantidade de classes no dataset\n",
    "# // K = 3\n",
    "\n",
    "\n",
    "\n",
    "# Função auxiliar\n",
    "def downloadFileIfNeeded(filePath, url):\n",
    "    if not os_path_exists(filePath):\n",
    "        with urlopen(url) as f:\n",
    "            html = f.read().decode('utf-8')\n",
    "        with open(filePath, 'w') as f:\n",
    "            f.write(html)\n",
    "\n",
    "\n",
    "\n",
    "datasetUrl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "datasetFilePath = './iris.data'\n",
    "columnNames = ['sepalLen', 'sepalWid', 'petalLen', 'petalWid', 'class']\n",
    "\n",
    "# Baixando dataset diretamente da internet, se necessário\n",
    "downloadFileIfNeeded(datasetFilePath, datasetUrl)\n",
    "\n",
    "# Lendo dataset do arquivo\n",
    "with open(datasetFilePath, 'r') as datasetFile:\n",
    "    dataset = pd.read_csv(datasetFilePath, names=columnNames)\n",
    "\n",
    "# # # ! Descomente para exibir\n",
    "# print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a coluna \"class\" em números (0, 1, …)\n",
    "classFactorized = dataset['class'].factorize()\n",
    "# Salvando os nomes de classe na ordem usada pelo .factorize()\n",
    "classes = list(classFactorized[1])\n",
    "# // # Sobrescrevendo a coluna do dataset pelos indexes obtidos\n",
    "# // dataset['class'] = classFactorized[0]\n",
    "\n",
    "# # # ! Descomente para exibir\n",
    "# print(f'##### Dataset #####\\n{dataset}\\n\\n')\n",
    "\n",
    "# Ignorando a coluna \"class\"\n",
    "datasetTreated = dataset.drop(['class'], axis=1)\n",
    "# Normalizando o dataset (normalização min-max), para que todos valores estejam no intervalo [1, 10]\n",
    "datasetTreated = ((datasetTreated - datasetTreated.min()) / (datasetTreated.max() - datasetTreated.min())) * 9 + 1\n",
    "\n",
    "# # # ! Descomente para exibir\n",
    "# print(f'##### Dataset (tratado e normalizado, intervalo [1, 10]) #####\\n{datasetTreated}')\n",
    "\n",
    "# Inferindo número de grupos pela quantidade de classes no dataset\n",
    "K = len(classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ! Descomente para rodar de fato\n",
    "# # Rodando kMeansCPU\n",
    "# results = kMeansGPU(datasetTreated, K, MAX_ITERATIONS)\n",
    "\n",
    "# results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conferindo Resultados\n",
    "\n",
    "Com a tabela `ClosestCentroids` em mãos, temos a classificação de cada instância do dataset *Iris* e, assim, podemos conferir os resultados obtidos através do k-means.\n",
    "\n",
    "Para isto, basta comparar a segunda coluna da tabela devolvida pelo `kMeansCPU()` com a última coluna do dataset Iris, que informa a classe correta de cada instância.\n",
    "\n",
    "Essa comparação, no entanto, não é tão simplória de se realizar. Não há como sabermos qual o mapeamento correto entre a classificação feita pelo k-means e as classes reais das instâncias do dataset. Isto é, não há como discernir se o `1` na coluna de classificação de centróides se refere a uma identificação de uma `Iris-versicolor` ao invés de uma `Iris-virginica`, por exemplo.\n",
    "\n",
    "Por isso, temos que realizar o **maximum matching** atingido pelo resultado do k-means, para realizar uma avaliação justa de seu desempenho.\n",
    "\n",
    "### Maximum Matching\n",
    "\n",
    "Teremos que avaliar todas as possibilidades de *mapeamento centróide–classe* e considerar apenas aquela que gera o melhor resultado.\n",
    "\n",
    "Para isso, é necessário calcular os acertos do resultado diversas vezes. Como há $ k! $ maneiras de se interpretar `K` centróides, isto irá gerar um **custo computacional de $ n \\cdot k! $ operações de comparação** entre classificações.\n",
    "\n",
    "Podemos reduzir este tempo de computação salvando e reaproveitando inteligentemente a quantidade de acertos de classificação computados para cada *mapeamento centróide–classe*.\n",
    "\n",
    "Por exemplo, há **duas** interpretações dos `K` centróides que possuem um mapeamento `0 -> Iris-Setosa`. São eles: `[Iris-setosa, Iris-versicolor, Iris-virginica]` e `[Iris-setosa, Iris-virginica, Iris-versicolor]`. Ao contar os acertos da primeira destas permutações dos `K` centróides, vamos obter os acertos de linhas do resultado do k-means onde o centróide mais próximo é o de index `0`, e cuja interpreção atual é de este `0` indicar uma flor `Iris-Setosa`. Como iremos usar essa mesma interpretação para centróides de index `0` na próxima permutação, podemos pular essa contagem redundante utilizando o resultado obtido na contagem anterior.\n",
    "\n",
    "Utilizando este método, conseguimos **reduzir o custo de computação** de acertos de classificação **para $ n^2 $ operações de comparação**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "# # # ! Descomente para rodar de fato\n",
    "# print(f'result:\\n{results}\\n')\n",
    "# print(f'dataset:\\n{dataset}\\n')\n",
    "# print(f'classes:\\n{classes}\\n')\n",
    "\n",
    "# # Converting from numpy arrays to panda's dataframes, if needed\n",
    "# if results.__class__.__name__ != pd.DataFrame.__class__.__name__: results = pd.DataFrame(results)\n",
    "\n",
    "def getClassificationHits(results:pd.DataFrame, dataset:pd.DataFrame, classColumn:str|int='class', classes:list[str]=None, debug=False):\n",
    "    '''Função auxiliar que retorna uma tupla com três informações: (1) a quantidade de acertos de classificação expressos no dataframe `results`; (2) as quantidades destes acertos separadas por classes; e (3) a interpretação das classes usadas para encontrar o melhor resultado\n",
    "    \n",
    "    Para determinar acertos e erros, é usada a coluna de nome/index `classColumn` do dataframe `dataset` como fonte de verdade. O maximum matching é feito para encontrar a melhor interpretação dos resultados e usá-la como resultado final\n",
    "\n",
    "    `results` deve ser um dataframe com duas colunas, a primeira sendo o index do datapoint e a segunda sendo o index da classe à qual o datapoint foi classificado\n",
    "    \n",
    "    Uma lista com os nomes das classes pode ser passada em `classes`, para agilizar o processo. Se nada for passado, as classes serão inferidas pela coluna de nome/index `classColumn` do dataframe `dataset`\n",
    "\n",
    "    Exemplo de retorno: `(118, {'Iris-setosa': 35, 'Iris-versicolor': 39, 'Iris-virginica': 44}, ('Iris-virginica', 'Iris-setosa', 'Iris-versicolor'))`\n",
    "    \n",
    "    O retorno acima significa que (1) houveram `118` acertos totais, (2) sendo `35` desses da classe `Iris-setosa`, `39` da classe `Iris-versicolor` e `44` da classe `Iris-virginica` e (3) que a interpretação usada para obter este melhor resultado foi: classe `0` em `results` = `Iris-virginica`, classe `1` = `Iris-setosa` e  classe `2` = `Iris-versicolor`.'''\n",
    "\n",
    "    def getHitsForClassAndPosition(class_:str, position:int, results:pd.DataFrame=results, dataset:pd.DataFrame=dataset, classColumn:str|int=classColumn):\n",
    "        '''Função auxiliar interna que calcula os acertos de uma única interpretação de uma única classe\n",
    "        \n",
    "        Tal interpretação é expressa em duas variáveis: `class_` -> nome da classe (assim como está expressa na coluna `classColumn` do `dataset`) e `position` -> posição da classe quanto ao array das possíveis k classes ([0, …, k-1] da segunda coluna de `results`)\n",
    "        \n",
    "        Exemplo:\n",
    "        \n",
    "        `getHitsForClassAndPosition('Iris-setosa', 1)` irá calcular os acertos em `results` ao considerar a classificação de index `1` como simbolizando a classe `Iris-setosa`'''\n",
    "\n",
    "        hits = 0\n",
    "\n",
    "        print(f'Counting hits for class \"{class_}\" being index \"{position}\" in the results dataset...')\n",
    "\n",
    "        for rowIndex in range(0, len(results)):\n",
    "            row = results.iloc[[rowIndex]]\n",
    "            # datapointIndex = row.index.values[0]\n",
    "            resultClassIndex = row.values[0]\n",
    "\n",
    "            # isCorrect = None\n",
    "            # correctClass = None\n",
    "\n",
    "            # Conferimos apenas linhas onde o index em \"results\" é o mesmo em \"position\", pois só nos importamos com acertos para palpites deste index\n",
    "            if resultClassIndex == position:\n",
    "                # Obtendo a classe correta da nossa \"fonte de verdade\", o dataset \"dataset\"\n",
    "                correctClass = dataset.iloc[[rowIndex]][classColumn].values[0]\n",
    "\n",
    "                # Checando se foi um acerto\n",
    "                isCorrect = True if str(correctClass) == str(class_) else False\n",
    "                if isCorrect: hits += 1\n",
    "                \n",
    "            # print(f'dpIndex = {datapointIndex}; result = {resultClassIndex}; rowDataset = {correctClass}; isCorrect = {isCorrect}')\n",
    "\n",
    "        print(f'Total hits: {hits}\\n')\n",
    "        return hits\n",
    "        return np.random.randint(0, 50 + 1)\n",
    "\n",
    "    if debug: print('#################### Computing classification hits... ####################\\n')\n",
    "\n",
    "    if classes is None or len(classes) == 0:\n",
    "        classes = list(dataset[classColumn].factorize()[1])\n",
    "        if debug: print(f'classes (inferred from `dataset`):\\n{classes}\\n')\n",
    "\n",
    "    # Foçar classes para strings\n",
    "    classes = [str(c) for c in classes]\n",
    "\n",
    "    # Encontrar todas as permutações possíveis das classes. A posição de uma classe na permutação será usada para atrelar uma classe ao centroid de mesmo index. Por exemplo, na permutação ('Iris-versicolor', 'Iris-setosa', 'Iris-virginica'), o centróide 0 corresponderá à classe 'Iris-versicolor', o centróide 1 à classe 'Iris-setosa' e o centróide 2 à classse 'Iris-virginica'. Cada datapoint terá sua classificação definida através dessa relação\n",
    "    classPerm = list(permutations(classes))\n",
    "\n",
    "    if debug:\n",
    "        print(f'classesPermutatons (len={len(classPerm)}):\\n')\n",
    "        for perm in classPerm: print(perm)\n",
    "        print('\\n')\n",
    "\n",
    "    # Dicionário auxiliar que guarda a quantidade de acertos por classe e posição na lista de classes. Isto economizará poder computacional, pois não repetiremos cálculos redundantes de hits de uma classe numa mesma posição.\n",
    "    # Estrutura:\n",
    "    # {\n",
    "    #     'Iris-setosa/0': 50,\n",
    "    #     'Iris-setosa/1': 0,\n",
    "    #     '<class>/<positionInPermutation>': 45,…\n",
    "    # }\n",
    "    hitsByclassAndPosition = {}\n",
    "\n",
    "    bestHits = -1\n",
    "    bestHitsPerClass = None\n",
    "    bestPerm = None\n",
    "\n",
    "    # Para cada permutação das classes\n",
    "    for permutation in classPerm:\n",
    "        # print(f'permutation = {permutation}\\n')\n",
    "        hitsPerClass = dict.fromkeys(classes, -1)\n",
    "\n",
    "        # Para cada classe na permutação atual\n",
    "        for classPosition, class_ in enumerate(permutation):\n",
    "            # Se os hits para essa classe nessa posição da permutação já foram computados, vamos usá-los. Se não, os computamos pela primeira vez e salvamos para usos posteriores\n",
    "            classAndPositionStr = f'{class_}/{classPosition}' # Chave a ser usada no dicionário\n",
    "            hits:int = hitsByclassAndPosition.get(classAndPositionStr, None)\n",
    "            if hits is None:\n",
    "                # TODO\n",
    "                # TODO\n",
    "                # TODO: Implementar o cálculo real dos acertos aqui\n",
    "                # TODO\n",
    "                # TODO\n",
    "                hits = getHitsForClassAndPosition(class_, classPosition)\n",
    "                hitsByclassAndPosition[classAndPositionStr] = hits\n",
    "\n",
    "            # Salvando a quantidade de acertos na classe atual, seja qual for\n",
    "            hitsPerClass[class_] = hits\n",
    "            # print(f'classPosition = {classPosition}; class_ = {class_}\\nhitsPerClass = {hitsPerClass}\\nhitsByclassAndPosition = {hitsByclassAndPosition}\\n')\n",
    "\n",
    "        # Computar acertos totais dessa permutação\n",
    "        totalHits = sum(hitsPerClass.values())\n",
    "\n",
    "        if totalHits > bestHits:\n",
    "            bestHits = totalHits\n",
    "            bestHitsPerClass = hitsPerClass\n",
    "            bestPerm = permutation\n",
    "\n",
    "        print(f'totalHits = {totalHits}\\nbestHits = {bestHits}; bestHitsPerClass = {bestHitsPerClass}; bestPerm = {bestPerm}\\n\\n')\n",
    "\n",
    "    return (bestHits, bestHitsPerClass, bestPerm)\n",
    "\n",
    "\n",
    "\n",
    "# # # ! Descomente para rodar de fato\n",
    "# hits, hitsPerClass, centroidClasses  = getClassificationHits(results, dataset, classes=classes, debug=DEBUG)\n",
    "\n",
    "# if DEBUG: print(f'Hits: {hits}\\nHits per class: {hitsPerClass}\\nCentroid classes: {centroidClasses}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "Rodando a função **`getClassificationHits()`** nos resultados do k-means, encontramos resultados bem esperados:\n",
    "\n",
    "- Acertos totais entre **130–140**, de 150 classificações (**86,66–93,33%** de acerto)\n",
    "- Acertos por classe:\n",
    "  - `Iris-setosa`: quase sempre **50** de 50 (**100%**)\n",
    "  - `Iris-versicolor`: **35–38** de 50 (**70–76%**)\n",
    "  - `Iris-virginica`: **45–48** de 50 (**90–96%**)\n",
    "\n",
    "Estes resultados são exatamente o que esperamos do dataset **Iris**, já que nele temos uma classe linearmente separável (`Iris-setosa`) e duas que NÃO são linearmente separáveis uma da outra (`Iris-versicolor` e `Iris-virginica`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
